{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5SF-11XKMrc"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import base64\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') \n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuzXxfB99YTl"
   },
   "source": [
    "## Task 01: Implementing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv2D1 = tfkl.Conv2D(filters=16, kernel_size=3, strides=(2,2), padding=\"valid\", activation=\"relu\") #, kernel_initializer=tf.initializers.glorot_normal(), bias_initializer=tf.initializers.constant(0.1)\n",
    "        self.conv2D2 = tfkl.Conv2D(filters=32, kernel_size=3, strides=(2,2), padding=\"valid\", activation=\"relu\")\n",
    "        self.flatten = tfkl.Flatten()\n",
    "        self.dense = tfkl.Dense(units=64, activation=\"relu\")\n",
    "        self.out1 = tfkl.Dense(units=1, activation = \"tanh\")\n",
    "        self.out2 = tfkl.Dense(units=env.action_space.shape[0]-1, activation = \"sigmoid\") # oder rescalen\n",
    "\n",
    "        \n",
    "    @tf.function()\n",
    "    def call(self, states, actions=None):\n",
    "        \n",
    "        x = self.conv2D1(states)\n",
    "        x = self.conv2D2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        mus1 = self.out1(x)\n",
    "        mus2 = self.out2(x)\n",
    "        mus = tf.concat((mus1, mus2), axis=1)\n",
    "\n",
    "        multivariate = tfp.distributions.MultivariateNormalDiag(loc=mus, scale_diag=[[0.5, 0.2, 0.2]])\n",
    "        \n",
    "        if actions==None:\n",
    "            actions = multivariate.sample()\n",
    "            log_probs = multivariate.log_prob(actions)\n",
    "            return actions, log_probs\n",
    "        else:\n",
    "            log_probs = multivariate.log_prob(actions)\n",
    "            return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWyblwWzOGqt"
   },
   "source": [
    "## Task 02: Creating Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectory(env, model, discount_factor, return_avg_return: bool=False, steps=None, num_trajectories: int=1):\n",
    "    \n",
    "    s = []\n",
    "    a = []\n",
    "    r = []\n",
    "    g = []\n",
    "    s_prime = []\n",
    "    time_steps = []\n",
    "    \n",
    "    avg_return = 0.0\n",
    "    \n",
    "    for _ in range(num_trajectories):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        cumul_return = 0.0\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        \n",
    "        r_cache = []\n",
    "\n",
    "        while not (done or step_count==steps):\n",
    "            actions, _ = model(tf.expand_dims(tf.convert_to_tensor(obs/255, dtype=tf.float32), axis=0))\n",
    "            obs_prime, reward, done, _ = env.step(tf.squeeze(actions, axis=0).numpy())\n",
    "            \n",
    "            s.append(tf.convert_to_tensor(obs/255, dtype=tf.float32))\n",
    "            a.append(actions)\n",
    "            r.append(reward)\n",
    "            r_cache.append(reward)\n",
    "            s_prime.append(tf.convert_to_tensor(obs_prime/255, dtype=tf.float32))\n",
    "            \n",
    "            \n",
    "            obs = obs_prime\n",
    "            time_steps.append(step_count)\n",
    "            step_count+=1\n",
    "    \n",
    "        g_cache = []\n",
    "        \n",
    "        for idx, reweward in enumerate(reversed(r_cache)):\n",
    "            cumul_return = discount_factor*cumul_return + reweward\n",
    "            g_cache.append(cumul_return)\n",
    "            \n",
    "        avg_return += g_cache[-1]\n",
    "        \n",
    "        g.extend(reversed(g_cache))\n",
    "        \n",
    "    if return_avg_return:\n",
    "        return  tf.data.Dataset.from_tensor_slices((s,a,r,g,s_prime, time_steps)), avg_return/num_trajectories\n",
    "    else:\n",
    "        return  tf.data.Dataset.from_tensor_slices((s,a,r,g,s_prime, time_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 03: Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQiJa1qiaSJI"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def gradient_step(model, s, a, g, time_steps, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = - tf.math.reduce_mean(model(states=s, actions=tf.squeeze(a, axis=1))*(g*discount_factor**time_steps))\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(path):\n",
    "    \"\"\"Function to display an mp4 file in the current notebook\n",
    "    \n",
    "     Arguments:\n",
    "            path (str): Directory from where the video should be loaded\n",
    "            \n",
    "    Returns:\n",
    "            (ipythondisplay.HTML): Create a display object given raw data.\n",
    "    \"\"\"\n",
    "    \n",
    "    video = open(path,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''<video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "\n",
    "    return ipythondisplay.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(env, model, path: str, num_episodes: int=1, fps: int=30):\n",
    "    \"\"\"Function to save an mp4 file of the given models performance\n",
    "    \n",
    "     Arguments:\n",
    "            env (gym): Environment the agent interacts with\n",
    "            policy (Policy): Policy to perform the actions in the environment\n",
    "            model_network (tf.keras.Model): Model to calculate the Q-values\n",
    "            path (str): Directory to which the video should be saved to\n",
    "            num_episodes (int): Number of to be saved episodes \n",
    "            fps (int): Frames per second of the to be saved video\n",
    "    \"\"\"\n",
    "    \n",
    "    path += \".mp4\"\n",
    "    with imageio.get_writer(path, fps=fps) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            video.append_data(env.render(mode='rgb_array'))\n",
    "            done = False\n",
    "            while not done:\n",
    "                actions, _ = model(tf.expand_dims(tf.convert_to_tensor(obs/255, dtype=tf.float32), axis=0))\n",
    "                obs, _, done, _  = env.step(tf.squeeze(actions, axis=0).numpy())\n",
    "                video.append_data(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(start):\n",
    "    \"\"\"Function to time the duration of each epoch\n",
    "\n",
    "    Arguments:\n",
    "        start (time): Start time needed for computation \n",
    "    \n",
    "    Returns:\n",
    "        time_per_training_step (time): Rounded time in seconds \n",
    "    \"\"\"\n",
    "    \n",
    "    now = time.time()\n",
    "    time_per_training_step = now - start\n",
    "    \n",
    "    return round(time_per_training_step, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV2gtYrActPY"
   },
   "outputs": [],
   "source": [
    "def train_CNN(env, model, steps, num_trajectories, num_epochs: int, batch_size: int, learning_rate: float, discount_factor: float):\n",
    "\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    episode_rewards = []\n",
    "    cumul_reward = 0.0\n",
    "    step_counter = 0\n",
    "\n",
    "    create_policy_eval_video(env, model, f\"Videos/eval_vid_epoch_pretraining\")\n",
    "    model.save_weights(f\"CarRacingCNNWeights_epoch_pretraining\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_average = 0\n",
    "        avg_return = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        dataset, avg_return = create_trajectory(env, model, discount_factor=discount_factor, return_avg_return=True, steps=steps, num_trajectories=num_trajectories)\n",
    "        \n",
    "        dataset = dataset.cache().shuffle(buffer_size=len(dataset)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        num_steps = 0\n",
    "        \n",
    "        episode_rewards.append(avg_return)\n",
    "\n",
    "        for s, a, _, g, _, time_steps in dataset.take(1):\n",
    "            loss = gradient_step(model, s, a, g, time_steps, optimizer)\n",
    "            running_average = 0.95 * running_average + (1 - 0.95) * loss\n",
    "\n",
    "        \n",
    "        train_losses.append(float(running_average))\n",
    "                            \n",
    "        # Create videos of performance and save them\n",
    "        if epoch%10 == 0:\n",
    "            create_policy_eval_video(env, model, f\"Videos/eval_vid_epoch_{epoch}\")\n",
    "            model.save_weights(f\"CarRacingCNNWeights_epoch_{epoch}\")\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"The last epoch took: {timing(start)} seconds\")\n",
    "        print()\n",
    "        fig1, ax1 = plt.subplots(nrows=1, ncols=2, figsize = (20, 6))\n",
    "        ax1[0].plot(train_losses)\n",
    "        ax1[0].set(ylabel='Loss', xlabel='Epoch', title=f'Average loss over {epoch} epochs')\n",
    "        ax1[1].plot(episode_rewards)\n",
    "        ax1[1].set(ylabel='Return', xlabel='Epoch', title=f'Average return over {epoch} epochs')\n",
    "        plt.show()\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "R_CNJ0rQ7LI0",
    "outputId": "c365a8b1-e5fb-40a1-d479-6f5e6e98d725",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "discount_factor = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 500\n",
    "steps = None\n",
    "num_trajectories = 3\n",
    "\n",
    "env = gym.make(\"CarRacing-v1\")\n",
    "\n",
    "model = CNN(env)\n",
    "\n",
    "train_CNN(env=env, model=model, steps=steps, num_trajectories=num_trajectories, num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate, discount_factor=discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C - PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DVN(tf.keras.Model):\n",
    "    \"\"\"DVN to calculate state values given the current state and action\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(DVN, self).__init__()\n",
    "        \n",
    "        self.conv2D1 = tfkl.Conv2D(filters=16, kernel_size=3, strides=(2,2), padding=\"valid\", activation=\"relu\")\n",
    "        self.conv2D2 = tfkl.Conv2D(filters=32, kernel_size=3, strides=(2,2), padding=\"valid\", activation=\"relu\")\n",
    "        self.flatten = tfkl.Flatten()\n",
    "        self.dense = tfkl.Dense(units=64, activation=\"relu\")\n",
    "        self.out = tfkl.Dense(units=1)\n",
    "        \n",
    "        \n",
    "    @tf.function()\n",
    "    def call(self, states):\n",
    "        \n",
    "        x = self.conv2D1(states)\n",
    "        x = self.conv2D2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        out = self.out(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def gradient_step(actor, critic, s, a, r, g, s_prime, ppo_steps: int, discount_factor: float, eps: float, actor_optimizer, critic_optimizer):\n",
    "\n",
    "    log_old = actor(states = s, actions = tf.squeeze(a, axis=1))\n",
    "    advantages = (r + discount_factor*critic(states=s_prime)) - critic(states=s)\n",
    "    \n",
    "    \n",
    "    for _ in range(ppo_steps):\n",
    "        \n",
    "        with tf.GradientTape() as actor_tape:  \n",
    "            log_new = actor(states = s, actions = tf.squeeze(a, axis=1))\n",
    "            ratios = tf.math.exp(log_new)/tf.math.exp(log_old)\n",
    "            actor_loss = - tf.reduce_mean(tf.math.minimum(x = ratios*advantages, y=tf.clip_by_value(ratios, 1.0-eps, 1.0+eps)*advantages))\n",
    "            \n",
    "        gradients = actor_tape.gradient(actor_loss, actor.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(gradients, actor.trainable_variables))\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as critic_tape:\n",
    "        critic_loss = tf.keras.losses.mean_squared_error(tf.squeeze(critic(states=s)), g)\n",
    "    \n",
    "    gradients = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(gradients, critic.trainable_variables))    \n",
    "    \n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(env, actor, critic, steps, num_trajectories, num_epochs: int, ppo_steps: int, batch_size: int, learning_rate: float, discount_factor: float, eps: float):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    critic_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    actor_train_losses = []\n",
    "    critic_train_losses = []\n",
    "    episode_rewards = []\n",
    "    cumul_reward = 0.0\n",
    "    step_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        actor_running_average = 0\n",
    "        critic_running_average = 0\n",
    "        avg_return = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        dataset, avg_return = create_trajectory(env, actor, discount_factor=discount_factor, return_avg_return=True, steps=steps, num_trajectories=num_trajectories)\n",
    "        \n",
    "        dataset = dataset.cache().shuffle(buffer_size=len(dataset)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        num_steps = 0\n",
    "        \n",
    "        episode_rewards.append(avg_return)\n",
    "\n",
    "        for s, a, r, g, s_prime, _ in dataset.take(1):\n",
    "            actor_loss, critic_loss = gradient_step(actor=actor, critic=critic, s=s, a=a, r=r, g=g, s_prime=s_prime, ppo_steps=ppo_steps, \n",
    "                                 discount_factor=discount_factor, eps=eps, actor_optimizer=actor_optimizer, critic_optimizer=critic_optimizer)\n",
    "            # Calculate running average to smoothen loss plot\n",
    "            actor_running_average = 0.95 * actor_running_average + (1 - 0.95) * actor_loss\n",
    "            critic_running_average = 0.95 * critic_running_average + (1 - 0.95) * critic_loss\n",
    "\n",
    "        actor_train_losses.append(float(actor_running_average))\n",
    "        critic_train_losses.append(float(critic_running_average))\n",
    "                            \n",
    "        # Create videos of performance and save them\n",
    "        if epoch%20 == 0:\n",
    "            create_policy_eval_video(env, actor, f\"Videos/PPO_eval_vid_epoch_{epoch}\")\n",
    "            actor.save_weights(f\"CarRacingPPOWeights_epoch_{epoch}\")\n",
    "            critic.save_weights(f\"CarRacingPPOWeights_epoch_{epoch}\")\n",
    "                    \n",
    "        clear_output()\n",
    "        print(f\"The last epoch took: {timing(start)} seconds\")\n",
    "        print()\n",
    "        fig1, ax1 = plt.subplots(nrows=1, ncols=2, figsize = (20, 6))\n",
    "        ax1[0].plot(actor_train_losses, label=\"actor\")\n",
    "        ax1[0].plot(critic_train_losses, label=\"critic\")\n",
    "        ax1[0].set(ylabel='Loss', xlabel='Epoch', title=f'Average loss over {epoch} epochs')\n",
    "        ax1[1].plot(episode_rewards)\n",
    "        ax1[1].set(ylabel='Return', xlabel='Epoch', title=f'Average return over {epoch} epochs')\n",
    "        ax1[0].legend()\n",
    "        plt.show()\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "discount_factor = 0.999\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "steps = None\n",
    "num_trajectories = 3\n",
    "ppo_steps = 10\n",
    "eps = 0.1\n",
    "\n",
    "env = gym.make(\"CarRacing-v1\")\n",
    "\n",
    "actor = CNN(env)\n",
    "critic = DVN(env)\n",
    "\n",
    "train_PPO(env=env, actor=actor, critic=critic, steps=steps, num_trajectories=num_trajectories, num_epochs=num_epochs, \n",
    "          ppo_steps=ppo_steps, batch_size=batch_size, learning_rate=learning_rate, discount_factor=discount_factor, eps=0.1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_HA3_LunarV2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
